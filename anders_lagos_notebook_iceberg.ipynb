{
	"metadata": {
		"toc-autonumbering": false,
		"kernelspec": {
			"name": "glue_pyspark",
			"display_name": "Glue PySpark",
			"language": "python"
		},
		"language_info": {
			"name": "Python_Glue_Session",
			"mimetype": "text/x-python",
			"codemirror_mode": {
				"name": "python",
				"version": 3
			},
			"pygments_lexer": "python3",
			"file_extension": ".py"
		}
	},
	"nbformat_minor": 4,
	"nbformat": 4,
	"cells": [
		{
			"cell_type": "code",
			"source": "%idle_timeout 2880\n%glue_version 3.0\n%worker_type G.1X\n%number_of_workers 2\n%%configure \n{\n  \"--conf\": \"spark.sql.extensions=org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\",\n  \"--datalake-formats\": \"iceberg\"\n}",
			"metadata": {
				"trusted": true
			},
			"execution_count": 12,
			"outputs": [
				{
					"name": "stdout",
					"text": "Current idle_timeout is 2880 minutes.\nidle_timeout has been set to 2880 minutes.\nSetting Glue version to: 3.0\nPrevious worker type: G.1X\nSetting new worker type to: G.1X\nPrevious number of workers: 2\nSetting new number of workers to: 2\nThe following configurations have been updated: {'--conf': 'spark.sql.extensions=org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions', '--datalake-formats': 'iceberg'}\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "### create the variables",
			"metadata": {
				"tags": []
			}
		},
		{
			"cell_type": "code",
			"source": "catalog_name = \"glue_catalog\"\nbucket_name = \"andres-lagos-bucket\"\nbucket_prefix = \"iceberg\"\ndatabase_name = \"dictionary_quality\"\nwarehouse_path = f\"s3://{bucket_name}/{bucket_prefix}/\"",
			"metadata": {
				"trusted": true
			},
			"execution_count": 1,
			"outputs": [
				{
					"name": "stdout",
					"text": "Authenticating with environment variables and user-defined glue_role_arn: arn:aws:iam::393747608406:role/service-role/AWSGlueServiceRole-Andres-Lagos\nTrying to create a Glue session for the kernel.\nWorker Type: G.1X\nNumber of Workers: 2\nSession ID: 46676e59-dd8f-493e-ae12-ccef603a2dd0\nJob Type: glueetl\nApplying the following default arguments:\n--glue_kernel_version 0.37.3\n--enable-glue-datacatalog true\n--conf spark.sql.extensions=org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\n--datalake-formats iceberg\nWaiting for session 46676e59-dd8f-493e-ae12-ccef603a2dd0 to get into ready status...\nSession 46676e59-dd8f-493e-ae12-ccef603a2dd0 has been created.\n\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "import pandas as pd\nimport re\nimport sys\nfrom awsglue.utils import getResolvedOptions\nfrom pyspark.sql import SparkSession\nfrom awsglue.transforms import *\nfrom awsglue.utils import getResolvedOptions\nfrom pyspark.context import SparkContext\nfrom awsglue.context import GlueContext\nfrom awsglue.job import Job\n\nspark = SparkSession.builder \\\n    .config(\"spark.sql.warehouse.dir\", warehouse_path) \\\n    .config(f\"spark.sql.catalog.{catalog_name}\", \"org.apache.iceberg.spark.SparkCatalog\") \\\n    .config(f\"spark.sql.catalog.{catalog_name}.warehouse\", warehouse_path) \\\n    .config(f\"spark.sql.catalog.{catalog_name}.catalog-impl\", \"org.apache.iceberg.aws.glue.GlueCatalog\") \\\n    .config(f\"spark.sql.catalog.{catalog_name}.io-impl\", \"org.apache.iceberg.aws.s3.S3FileIO\") \\\n    .config(\"spark.sql.extensions\", \"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\") \\\n    .getOrCreate()",
			"metadata": {
				"editable": true,
				"tags": [],
				"trusted": true
			},
			"execution_count": 2,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "sys.argv.append('--JOB_NAME')\nsys.argv.append('andres_lagos_notebook_icerberg')\nargs = getResolvedOptions(sys.argv, ['JOB_NAME'])\nglueContext = GlueContext(spark)\njob = Job(glueContext)\njob.init(args['JOB_NAME'], args)",
			"metadata": {
				"trusted": true
			},
			"execution_count": 8,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "class Create_Dictionary_Tables():\n    \"Create the tables of dictionary\"\n    \n    def __init__(self, catalog_name: str, database_name: str) -> None:\n        \"\"\"\n        initialize the database parameters\n        Arg:\n            catalog_name: the glue data catalog name\n            database_name: the glye database name of dictionary data\n        \"\"\"\n        \n        self.database_name = database_name\n        self.catalog_name = catalog_name\n        self.warehouse_path = f\"s3://{bucket_name}/{bucket_prefix}\"\n    \n    def create_column_table(self, column_table_name: str = 'column') -> None:\n        \"\"\"\n        check and create the column table\n        Arg:\n            column_table_name: name of the column's table metadata\n        return:\n            None\n        \"\"\"\n        \n        try:\n            spark.table(f\"{self.catalog_name}.{self.database_name}.{column_table_name}\").limit(1).show()\n        except:\n            spark.sql(f\"\"\"\n                CREATE TABLE {self.catalog_name}.{self.database_name}.{column_table_name}(\n                    column_id string\n                    , table_id string\n                    , database string\n                    , table string\n                    , column_name string\n                    , description string\n                    , data_type string\n                    , nullable boolean\n                    , is_partitioned boolean\n                    , ordinal_position int\n                    , is_deleted boolean\n                    )\n                PARTITIONED BY (table_id)\n                LOCATION '{self.warehouse_path}/{column_table_name}'\n                TBLPROPERTIES (\n                  'table_type'='ICEBERG',\n                  'format'='parquet'\n                )\n                \"\"\").show()\n            \n    def create_table_table(self, table_table_name: str = 'table') -> None:\n        \"\"\"\n        check and create the table table\n        Arg:\n            table_table_name: name of the table's table metadata\n        return:\n            None\n        \"\"\"\n        \n        try:\n            response = spark.table(f\"{self.catalog_name}.{self.database_name}.{table_table_name}\").limit(1).show()\n        except:\n            spark.sql(f\"\"\"\n            CREATE TABLE {self.catalog_name}.{self.database_name}.{table_table_name}(\n                table_id string\n                , database string\n                , table string\n                , description string\n                , create_statement string\n                , update_crawler string\n                , are_columns_quoted boolean\n                , classification string\n                , columns_ordered string\n                , compression_type string\n                , delimiter string\n                , object_count int\n                , record_count int\n                , size_key int\n                , skip_line int\n                , type_data string\n                , location string\n                , is_view boolean\n                , is_deleted boolean\n                )\n                PARTITIONED BY (table_id)\n                LOCATION '{self.warehouse_path}/{table_table_name}'\n                TBLPROPERTIES (\n                'table_type'='ICEBERG',\n                'format'='parquet'\n                )\"\"\").show()\n        \n    def create_tables(self) -> None:\n        \"Execute all of the create table process\"\n        self.create_column_table()\n        self.create_table_table()\n        ",
			"metadata": {
				"trusted": true
			},
			"execution_count": 3,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "Create_Dictionary_Tables(catalog_name, database_name).create_tables()",
			"metadata": {
				"trusted": true
			},
			"execution_count": 4,
			"outputs": [
				{
					"name": "stdout",
					"text": "+--------------------+--------------------+------------+---------+--------------------+-----------+---------+--------+--------------+----------------+----------+\n|           column_id|            table_id|    database|    table|         column_name|description|data_type|nullable|is_partitioned|ordinal_position|is_deleted|\n+--------------------+--------------------+------------+---------+--------------------+-----------+---------+--------+--------------+----------------+----------+\n|data_quality.aust...|data_quality.aust...|data_quality|australia|individual-local-...|       null|   string|    true|         false|              12|     false|\n+--------------------+--------------------+------------+---------+--------------------+-----------+---------+--------+--------------+----------------+----------+\n\n+--------------------+------------+--------------+-----------+--------------------+--------------+------------------+--------------+---------------+----------------+---------+------------+------------+--------+---------+---------+--------+-------+----------+\n|            table_id|    database|         table|description|    create_statement|update_crawler|are_columns_quoted|classification|columns_ordered|compression_type|delimiter|object_count|record_count|size_key|skip_line|type_data|location|is_view|is_deleted|\n+--------------------+------------+--------------+-----------+--------------------+--------------+------------------+--------------+---------------+----------------+---------+------------+------------+--------+---------+---------+--------+-------+----------+\n|data_quality.dele...|data_quality|deleted_values|       null|CREATE VIEW `data...|          None|             false|          null|          false|            null|     null|           0|           0|       0|        0|     None|    None|   true|     false|\n+--------------------+------------+--------------+-----------+--------------------+--------------+------------------+--------------+---------------+----------------+---------+------------+------------+--------+---------+---------+--------+-------+----------+\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "class Land_Metastore():\n    \"\"\"Lands the metadata for schemas listed in the database dictionary.\n    \n    This function will create the temporary views column_metadata, \n    table_metadata and create_metadata. These tables describe the metadata\n    of the tables contained in the database list.\n    \n    Where a landed value is the string 'None', it represents a null value\n    (pandas doesnt support a null types).\n    \n\n    Args:\n        database_list (list): list of databases\n    \"\"\"\n    def __init__(self, database_list: list) -> None:\n        \"Initialize the process and execute all of the steps\"\n        \n        tables = []\n        # for each database determine table list using spark catalog\n        for database in database_list:\n            table_list = spark.catalog.listTables(database)\n            tables += [{'tableName': i.name, 'database': i.database\n                        , 'description':i.description, 'tableType':i.tableType\n                        , 'isTemporary':i.isTemporary} for i in table_list]\n\n        # we dont want to document temp tales, remove from list.\n        tables = [i for i in tables if not i['isTemporary']]\n        self.tables = tables\n\n    def get_column_df(self) -> pd.DataFrame:\n        \"Return pandas databframe of column metadata given list of tables.\"\n       \n        all_column_metadata=[]\n        for table in self.tables:\n            # collect column info\n            column_list = spark.catalog.listColumns(table['tableName'], table['database'])\n            # can infer ordinal position by incrementing a counter from 1 for each column in list.\n            ordinal_position = 1\n            for col in column_list: \n                # add each columns metadata to all_column_metadata list\n                column_metadata = [table['database'], table['tableName']]+[col[index] for index in range(len(col))]+[ordinal_position]\n                ordinal_position+=1\n                all_column_metadata.append(column_metadata)\n\n        # set column names\n        column_metadata_columns = ['database', 'table','columnName','description', 'dataType','nullable','isPartitioned','isBucket', 'ordinalPosition']\n        df_columns = pd.DataFrame(all_column_metadata, columns=column_metadata_columns)\n        # resolve missing values\n        df_columns.fillna('None', inplace=True)\n        return df_columns\n\n    def get_table_df(self) -> pd.DataFrame:\n        \"\"\"Return pandas dataframe of table metadata given list of tables.\"\"\"\n\n        all_table_metadata = []\n        for table in self.tables:\n            if table['tableType']!='VIEW':\n                table_metadata = list()\n                #collect result of describe detail sql command, collect as list and add to metadata list\n                table_data = spark.sql(f\"DESCRIBE EXTENDED {table['database']}.{table['tableName']}\").collect()\n                for index in table_data:\n                    if index.col_name in ['Database', 'Table', 'Table Properties'\n                        , 'Location', 'Storage Properties', 'Partition Provider']:\n                        if index.col_name == 'Table Properties':\n                            table_metadata.extend([value.split('=')[1] for value in re.split(r\",(?!,)\",index[1])])\n                        else:\n                            table_metadata.append(index[1])\n                table_metadata.append(False)\n                table_metadata.append(table['description'])\n                all_table_metadata.append(table_metadata)\n            else:\n                table_metadata = [table['database'], table['tableName'], None, None\n                  , None, None, '0', None\n                  , None, None, None, '0'\n                  , '0', '0', '0', None, None\n                  , None, None, True, table['description']]\n                all_table_metadata.append(table_metadata)\n        table_metadata_columns = ['database', 'table', 'crawlerDeserializerV', 'crawlerSerializarV' \n                                , 'updateCrawler', 'areColumnsQuoted', 'avgRecordSize', 'classification' \n                                , 'columnsOrdered', 'compressionType', 'delimiter', 'objectCount'\n                                , 'recordCount', 'sizeKey', 'skipLine', 'typeData', 'location'\n                                , 'storageProperties', 'PartitionProvider', 'is_view', 'description']\n        df_tables = pd.DataFrame(all_table_metadata,  columns=table_metadata_columns)\n        # drop some columns we dont need\n        df_tables.drop(labels = ['crawlerDeserializerV', 'crawlerSerializarV', 'storageProperties', 'PartitionProvider'], axis=1, inplace=True)\n        # resolve missing values\n        df_tables.fillna(value={'avgRecordSize': 0, 'objectCount': 0, 'recordCount': 0, \n            'sizeKey': 0, 'skipLine': 0}, inplace=True)\n        df_tables.fillna('None', inplace=True)\n        return df_tables\n\n\n    def get_create_df(self) -> pd.DataFrame:\n        \"\"\"Return pandas databframe of create table statements given list of tables.\"\"\"\n        \n        all_create_metadata = []\n        for table in self.tables:\n            create = spark.sql(f\"SHOW CREATE TABLE {table['database']}.{table['tableName']}\").collect()[0][0]\n            table_metadata = [table['database'], table['tableName']] + [create]\n            all_create_metadata.append(table_metadata)\n        create_metadata_columns = ['database', 'table','createStatement']\n        df_create = pd.DataFrame(all_create_metadata, columns=create_metadata_columns)\n        return df_create\n\n    def temp_view_from_pandas(self, df, view_name):\n        \"\"\"Create a temporary view from a pandas dataframe\"\"\"\n\n        spark_df = spark.createDataFrame(df)\n        spark_df.createOrReplaceTempView(view_name)\n        print(f\"Temporary table `{view_name}` created.\")\n        print(f\"Row count: {spark_df.count()}\")\n        \n    def main(self) -> None:\n        \"Execute all of the workflow\"\n        # collect metadata and land metadata for schema\n        metadata_df = self.get_column_df()\n        self.temp_view_from_pandas(metadata_df, 'Column_Metadata')\n        metadata_df = self.get_table_df()\n        self.temp_view_from_pandas(metadata_df, 'Table_Metadata')\n        metadata_df = self.get_create_df()\n        self.temp_view_from_pandas(metadata_df, 'Create_Metadata')",
			"metadata": {
				"trusted": true
			},
			"execution_count": 5,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "Land_Metastore(['data_quality']).main()",
			"metadata": {
				"tags": [],
				"trusted": true
			},
			"execution_count": 6,
			"outputs": [
				{
					"name": "stdout",
					"text": "Temporary table `Column_Metadata` created.\nRow count: 67\nTemporary table `Table_Metadata` created.\nRow count: 5\nTemporary table `Create_Metadata` created.\nRow count: 5\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "class Upsert_Metadata():\n    \"Upsert metadata tables\"\n    \n    def __init__(self, catalog_name: str, database_name: str) -> None:\n        \"\"\"\n        initialize the database parameters\n        Arg:\n            catalog_name: the glue data catalog name\n            database_name: the glue database name of dictionary data\n        \"\"\"\n        \n        self.database_name = database_name\n        self.catalog_name = catalog_name\n        self.warehouse_path = f\"s3://{bucket_name}/{bucket_prefix}\"\n        \n    def upsert_column_data(self, column_table_name: str = 'column') -> None:\n        \"\"\"\n        Upsert dictionary table column with all of the available data\n        Arg:\n            column_table_name: the name of the dictionary column's table\n        Return:\n            None\n        \"\"\"\n        \n        spark.sql(f\"\"\"\n            MERGE INTO {catalog_name}.{database_name}.{column_table_name}  AS glue\n                USING Column_Metadata AS temp\n                ON glue.database = temp.database\n                AND glue.table = temp.table \n                AND glue.column_name = temp.ColumnName\n                WHEN MATCHED THEN UPDATE SET\n                glue.description=CASE WHEN temp.description = 'None' THEN NULL ELSE temp.description END,\n                glue.ordinal_position=temp.ordinalPosition,\n                glue.data_type=temp.dataType,\n                glue.nullable=temp.nullable,\n                glue.is_partitioned=temp.isPartitioned,\n                glue.is_deleted=False\n            WHEN NOT MATCHED THEN INSERT\n            (column_id, table_id, database, table, column_name\n            , description, ordinal_position, data_type, nullable,\n            is_partitioned, is_deleted)\n            VALUES\n                (CONCAT_WS('.', temp.database, temp.table, temp.columnName),\n                CONCAT_WS('.', temp.database, temp.Table),\n                temp.database,\n                temp.table,\n                temp.columnName,\n                CASE WHEN temp.description = 'None' THEN NULL ELSE temp.description END,\n                temp.ordinalPosition,\n                temp.dataType,\n                temp.nullable,\n                temp.isPartitioned,\n                False\n                )\n            \"\"\").show()\n        \n    def update_deleted_column(self, column_table_name: str = 'column') -> None:\n        \"\"\"\n        update delete column in the column's dictionary table\n        Arg:\n            column_table_name: the name of the dictionary column's table\n        Return:\n            None\n        \"\"\"\n\n        spark.sql(f\"\"\"CREATE OR REPLACE TEMP VIEW deleted_values AS\n            SELECT glue.*\n            FROM {catalog_name}.{database_name}.{column_table_name} glue\n            ANTI JOIN Column_Metadata temp\n                ON glue.database = temp.database\n                AND glue.column_name = temp.columnName\n                AND glue.table = temp.table\n            WHERE glue.database IN (SELECT database FROM Column_Metadata)\n            \"\"\").show()\n        spark.sql(f\"\"\"\n            MERGE INTO {catalog_name}.{database_name}.{column_table_name} glue\n            USING deleted_values deleted\n            ON glue.database = deleted.database\n                AND glue.column_name = deleted.column_name\n                AND glue.table = deleted.table\n            WHEN MATCHED THEN UPDATE SET\n            glue.is_deleted=True\n            \"\"\").show()\n            \n    def create_table_metadata(self) -> None:\n        \"\"\"\n        Create table's metadata\n        Arg:\n            column_table_name: the name of the dictionary table\n        Return:\n            None\n        \"\"\"\n\n        spark.sql(\"\"\"\n        CREATE OR REPLACE TEMP VIEW stage_table_metadata AS\n        SELECT \n            table.database || '.' || table.table AS tableId,\n            table.database AS database,\n            table.table AS table,\n            CASE WHEN table.description ='None' THEN null ELSE table.description END AS description,\n            metadata.createStatement as create_statement,\n            table.updateCrawler as update_crawler,\n            CASE WHEN table.areColumnsQuoted IN ('None', 'none') THEN false ELSE cast(table.areColumnsQuoted as boolean) END AS are_columns_quoted,\n            CASE WHEN table.classification IN ('None', 'none') THEN null ELSE table.classification END AS classification,\n            CASE WHEN table.columnsOrdered IN ('None', 'none') THEN false ELSE cast(table.columnsOrdered AS boolean) END AS columns_ordered,\n            CASE WHEN table.compressionType IN ('None', 'none') THEN null ELSE table.compressionType END AS compression_type,\n            CASE WHEN table.delimiter ='None' THEN null ELSE table.delimiter END AS delimiter,\n            cast(table.objectCount AS integer) AS object_count,\n            cast(table.recordCount AS integer) AS record_count,\n            cast(table.sizeKey AS integer) AS size_key,\n            cast(table.skipLine AS integer) AS skip_line,\n            table.typeData AS type_data,\n            table.location,\n            is_view,\n            False AS is_deleted\n        FROM table_metadata table\n        JOIN create_metadata metadata\n            ON table.database = metadata.database\n            AND table.table = metadata.table\n        \"\"\").show()\n            \n    def upsert_table_data(self, table_table_name: str = 'table') -> None:\n        \"\"\"\n        Upsert dictionary table table with all of the available data\n        Arg:\n            column_table_table: the name of the dictionary table's table\n        Return:\n            None\n        \"\"\"\n\n        spark.sql(f\"\"\"\n        MERGE INTO {catalog_name}.{database_name}.{table_table_name} glue \n        USING stage_table_metadata  temp\n            ON glue.database = temp.database\n            AND glue.table = temp.table \n        WHEN MATCHED THEN UPDATE SET\n            glue.create_statement = temp.create_statement,\n            glue.update_crawler = temp.update_crawler,\n            glue.are_columns_quoted = temp.are_columns_quoted,\n            glue.classification = temp.classification,\n            glue.columns_ordered = temp.columns_ordered,\n            glue.compression_type = temp.compression_type,\n            glue.delimiter = temp.delimiter,\n            glue.object_count = temp.object_count,\n            glue.record_count = temp.record_count,\n            glue.size_key = temp.size_key,\n            glue.type_data = temp.type_data,\n            glue.location = temp.location,\n            glue.is_view = temp.is_view,\n            glue.is_deleted = temp.is_deleted\n        WHEN NOT MATCHED THEN INSERT *\n        \"\"\").show()\n            \n    def update_delete_tables(self, table_table_name: str = 'table') -> None:\n        \"\"\"\n        Get delete tables\n        Arg:\n            table_table_table: the name of the dictionary table's table\n        Return:\n            None\n        \"\"\"\n\n        spark.sql(f\"\"\"\n        CREATE OR REPLACE TEMP VIEW deleted_values AS\n        SELECT table.*\n        FROM {catalog_name}.{database_name}.{table_table_name} table\n        ANTI JOIN table_metadata temp\n            ON table.database = temp.database\n            AND table.table = temp.table\n        \"\"\").show()\n        spark.sql(f\"\"\"\n        MERGE INTO {catalog_name}.{database_name}.{table_table_name} table\n        USING deleted_values temp\n        ON table.database = temp.database\n            AND table.table = temp.table\n        WHEN MATCHED THEN UPDATE SET\n            table.is_deleted=True\n        \"\"\").show()\n    \n    def upsert_tables(self) -> None:\n        \"Execute the upsert process\"\n        self.upsert_column_data()\n        self.update_deleted_column()\n        self.create_table_metadata()\n        self.upsert_table_data()\n        self.update_delete_tables()",
			"metadata": {
				"trusted": true
			},
			"execution_count": 7,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "Upsert_Metadata(catalog_name, database_name).upsert_tables()",
			"metadata": {
				"trusted": true
			},
			"execution_count": 8,
			"outputs": [
				{
					"name": "stdout",
					"text": "++\n||\n++\n++\n",
					"output_type": "stream"
				}
			]
		}
	]
}