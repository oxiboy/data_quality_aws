{
	"metadata": {
		"toc-autonumbering": false,
		"kernelspec": {
			"name": "glue_pyspark",
			"display_name": "Glue PySpark",
			"language": "python"
		},
		"language_info": {
			"name": "Python_Glue_Session",
			"mimetype": "text/x-python",
			"codemirror_mode": {
				"name": "python",
				"version": 3
			},
			"pygments_lexer": "python3",
			"file_extension": ".py"
		}
	},
	"nbformat_minor": 4,
	"nbformat": 4,
	"cells": [
		{
			"cell_type": "code",
			"source": "%idle_timeout 2880\n%glue_version 3.0\n%worker_type G.1X\n%number_of_workers 2\n%%configure \n{\n  \"--conf\": \"spark.sql.extensions=org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\",\n  \"--datalake-formats\": \"iceberg\"\n}",
			"metadata": {
				"trusted": true
			},
			"execution_count": 6,
			"outputs": [
				{
					"name": "stdout",
					"text": "Welcome to the Glue Interactive Sessions Kernel\nFor more information on available magic commands, please type %help in any new cell.\n\nPlease view our Getting Started page to access the most up-to-date information on the Interactive Sessions kernel: https://docs.aws.amazon.com/glue/latest/dg/interactive-sessions.html\nInstalled kernel version: 1.0.2 \nCurrent idle_timeout is None minutes.\nidle_timeout has been set to 2880 minutes.\nSetting Glue version to: 3.0\nPrevious worker type: None\nSetting new worker type to: G.1X\nPrevious number of workers: None\nSetting new number of workers to: 2\nThe following configurations have been updated: {'--conf': 'spark.sql.extensions=org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions', '--datalake-formats': 'iceberg'}\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "### create the variables",
			"metadata": {
				"tags": []
			}
		},
		{
			"cell_type": "code",
			"source": "catalog_name = \"glue_catalog\"\nbucket_name = \"andres-lagos-bucket\"\nbucket_prefix = \"iceberg\"\ndatabase_name = \"dictionary_quality\"\nwarehouse_path = f\"s3://{bucket_name}/{bucket_prefix}/\"",
			"metadata": {
				"trusted": true
			},
			"execution_count": 1,
			"outputs": [
				{
					"name": "stdout",
					"text": "Trying to create a Glue session for the kernel.\nSession Type: glueetl\nWorker Type: G.1X\nNumber of Workers: 2\nSession ID: ee908911-7283-407c-808d-55dff5637e9e\nApplying the following default arguments:\n--glue_kernel_version 1.0.2\n--enable-glue-datacatalog true\n--conf spark.sql.extensions=org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\n--datalake-formats iceberg\nWaiting for session ee908911-7283-407c-808d-55dff5637e9e to get into ready status...\nSession ee908911-7283-407c-808d-55dff5637e9e has been created.\n\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "import pandas as pd\nimport re\nimport sys\nfrom awsglue.utils import getResolvedOptions\nfrom pyspark.sql import SparkSession\nfrom awsglue.transforms import *\nfrom awsglue.utils import getResolvedOptions\nfrom pyspark.context import SparkContext\nfrom awsglue.context import GlueContext\nfrom awsglue.job import Job\n\nspark = SparkSession.builder \\\n    .config(\"spark.sql.warehouse.dir\", warehouse_path) \\\n    .config(f\"spark.sql.catalog.{catalog_name}\", \"org.apache.iceberg.spark.SparkCatalog\") \\\n    .config(f\"spark.sql.catalog.{catalog_name}.warehouse\", warehouse_path) \\\n    .config(f\"spark.sql.catalog.{catalog_name}.catalog-impl\", \"org.apache.iceberg.aws.glue.GlueCatalog\") \\\n    .config(f\"spark.sql.catalog.{catalog_name}.io-impl\", \"org.apache.iceberg.aws.s3.S3FileIO\") \\\n    .config(\"spark.sql.extensions\", \"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\") \\\n    .getOrCreate()",
			"metadata": {
				"trusted": true,
				"editable": true,
				"tags": []
			},
			"execution_count": 2,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "sys.argv.append('--JOB_NAME')\nsys.argv.append('andres_lagos_notebook_icerberg')\nargs = getResolvedOptions(sys.argv, ['JOB_NAME'])\nglueContext = GlueContext(spark)\njob = Job(glueContext)\njob.init(args['JOB_NAME'], args)",
			"metadata": {
				"trusted": true
			},
			"execution_count": 3,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "class Create_Dictionary_Tables():\n    \"Create the tables of dictionary\"\n    \n    def __init__(self, catalog_name: str, database_name: str) -> None:\n        \"\"\"\n        initialize the database parameters\n        Arg:\n            catalog_name: the glue data catalog name\n            database_name: the glye database name of dictionary data\n        \"\"\"\n        \n        self.database_name = database_name\n        self.catalog_name = catalog_name\n        self.warehouse_path = f\"s3://{bucket_name}/{bucket_prefix}\"\n    \n    def create_column_table(self, column_table_name: str = 'column') -> None:\n        \"\"\"\n        check and create the column table\n        Arg:\n            column_table_name: name of the column's table metadata\n        return:\n            None\n        \"\"\"\n        \n        try:\n            spark.table(f\"{self.catalog_name}.{self.database_name}.{column_table_name}\").limit(1).show()\n        except:\n            spark.sql(f\"\"\"\n                CREATE TABLE {self.catalog_name}.{self.database_name}.{column_table_name}(\n                    column_id string\n                    , table_id string\n                    , database string\n                    , table string\n                    , column_name string\n                    , description string\n                    , data_type string\n                    , nullable boolean\n                    , is_partitioned boolean\n                    , ordinal_position int\n                    , is_deleted boolean\n                    )\n                PARTITIONED BY (table_id)\n                LOCATION '{self.warehouse_path}/{column_table_name}'\n                TBLPROPERTIES (\n                  'table_type'='ICEBERG',\n                  'format'='parquet'\n                )\n                \"\"\").show()\n            \n    def create_table_table(self, table_table_name: str = 'table') -> None:\n        \"\"\"\n        check and create the table table\n        Arg:\n            table_table_name: name of the table's table metadata\n        return:\n            None\n        \"\"\"\n        \n        try:\n            response = spark.table(f\"{self.catalog_name}.{self.database_name}.{table_table_name}\").limit(1).show()\n        except:\n            spark.sql(f\"\"\"\n            CREATE TABLE {self.catalog_name}.{self.database_name}.{table_table_name}(\n                table_id string\n                , database string\n                , table string\n                , description string\n                , create_statement string\n                , update_crawler string\n                , are_columns_quoted boolean\n                , classification string\n                , columns_ordered string\n                , compression_type string\n                , delimiter string\n                , object_count int\n                , record_count int\n                , size_key int\n                , skip_line int\n                , type_data string\n                , location string\n                , is_view boolean\n                , is_deleted boolean\n                )\n                PARTITIONED BY (table_id)\n                LOCATION '{self.warehouse_path}/{table_table_name}'\n                TBLPROPERTIES (\n                'table_type'='ICEBERG',\n                'format'='parquet'\n                )\"\"\").show()\n        \n    def create_tables(self) -> None:\n        \"Execute all of the create table process\"\n        self.create_column_table()\n        self.create_table_table()\n        ",
			"metadata": {
				"trusted": true
			},
			"execution_count": 4,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "Create_Dictionary_Tables(catalog_name, database_name).create_tables()",
			"metadata": {
				"trusted": true
			},
			"execution_count": 5,
			"outputs": [
				{
					"name": "stdout",
					"text": "+--------------------+-------------------+---------+---------+--------------------+-----------+---------+--------+--------------+----------------+----------+\n|           column_id|           table_id| database|    table|         column_name|description|data_type|nullable|is_partitioned|ordinal_position|is_deleted|\n+--------------------+-------------------+---------+---------+--------------------+-----------+---------+--------+--------------+----------------+----------+\n|data_lake.austral...|data_lake.australia|data_lake|australia|algorithm-marked-...|       null|  boolean|    true|         false|               1|     false|\n+--------------------+-------------------+---------+---------+--------------------+-----------+---------+--------+--------------+----------------+----------+\n\n+-------------------+---------+---------+-----------+--------------------+-----------------+------------------+--------------+---------------+----------------+---------+------------+------------+---------+---------+---------+--------------------+-------+----------+\n|           table_id| database|    table|description|    create_statement|   update_crawler|are_columns_quoted|classification|columns_ordered|compression_type|delimiter|object_count|record_count| size_key|skip_line|type_data|            location|is_view|is_deleted|\n+-------------------+---------+---------+-----------+--------------------+-----------------+------------------+--------------+---------------+----------------+---------+------------+------------+---------+---------+---------+--------------------+-------+----------+\n|data_lake.australia|data_lake|australia|       null|CREATE TABLE `dat...|pet_cat_australia|             false|           csv|           true|            null|        ,|           1|      879937|139910027|        1|    file]|s3://andres-lagos...|  false|     false|\n+-------------------+---------+---------+-----------+--------------------+-----------------+------------------+--------------+---------------+----------------+---------+------------+------------+---------+---------+---------+--------------------+-------+----------+\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "class Land_Metastore():\n    \"\"\"Lands the metadata for schemas listed in the database dictionary.\n    \n    This function will create the temporary views column_metadata, \n    table_metadata and create_metadata. These tables describe the metadata\n    of the tables contained in the database list.\n    \n    Where a landed value is the string 'None', it represents a null value\n    (pandas doesnt support a null types).\n    \n\n    Args:\n        database_list (list): list of databases\n    \"\"\"\n    def __init__(self, database_list: list) -> None:\n        \"Initialize the process and execute all of the steps\"\n        \n        tables = []\n        # for each database determine table list using spark catalog\n        for database in database_list:\n            table_list = spark.catalog.listTables(database)\n            tables += [{'tableName': i.name, 'database': i.database\n                        , 'description':i.description, 'tableType':i.tableType\n                        , 'isTemporary':i.isTemporary} for i in table_list]\n\n        # we dont want to document temp tales, remove from list.\n        tables = [i for i in tables if not i['isTemporary']]\n        self.tables = tables\n\n    def get_column_df(self) -> pd.DataFrame:\n        \"Return pandas databframe of column metadata given list of tables.\"\n       \n        all_column_metadata=[]\n        for table in self.tables:\n            # collect column info\n            column_list = spark.catalog.listColumns(table['tableName'], table['database'])\n            # can infer ordinal position by incrementing a counter from 1 for each column in list.\n            ordinal_position = 1\n            for col in column_list: \n                # add each columns metadata to all_column_metadata list\n                column_metadata = [table['database'], table['tableName']]+[col[index] for index in range(len(col))]+[ordinal_position]\n                ordinal_position+=1\n                all_column_metadata.append(column_metadata)\n\n        # set column names\n        column_metadata_columns = ['database', 'table','columnName','description', 'dataType','nullable','isPartitioned','isBucket', 'ordinalPosition']\n        df_columns = pd.DataFrame(all_column_metadata, columns=column_metadata_columns)\n        # resolve missing values\n        df_columns.fillna('None', inplace=True)\n        return df_columns\n\n    def get_table_df(self) -> pd.DataFrame:\n        \"\"\"Return pandas dataframe of table metadata given list of tables.\"\"\"\n        all_table_metadata = []\n        for table in self.tables:\n            if table['tableType']!='VIEW':\n                table_metadata = list()\n                #collect result of describe detail sql command, collect as list and add to metadata list\n                table_data = spark.sql(f\"DESCRIBE EXTENDED {table['database']}.{table['tableName']}\").collect()\n                for index in table_data:\n                    if index.col_name in ['Database', 'Table', 'Table Properties'\n                        , 'Location', 'Storage Properties', 'Partition Provider']:\n                        if index.col_name == 'Table Properties':\n                            table_metadata.extend([value.split('=')[1] for value in re.split(r\",(?!,)\",index[1])])\n                        else:\n                            table_metadata.append(index[1])\n                if len(table_metadata) == 19:\n                    table_metadata.insert(10, '0')\n                    table_metadata.insert(11, '')  \n                table_metadata.append(False)\n                table_metadata.append(table['description'])\n                all_table_metadata.append(table_metadata)\n            else:\n                table_metadata = [table['database'], table['tableName'], None, None\n                  , None, None, '0', None\n                  , None, None, None, '0'\n                  , '0', '0', '0', None, None\n                  , None, None, True, table['description']]\n                all_table_metadata.append(table_metadata)\n        table_metadata_columns = ['database', 'table', 'crawlerDeserializerV', 'crawlerSerializarV' \n                                , 'updateCrawler', 'areColumnsQuoted', 'avgRecordSize', 'classification' \n                                , 'columnsOrdered', 'compressionType', 'delimiter', 'last_modified_by'\n                                ,  'last_modified_time', 'objectCount'\n                                , 'recordCount', 'sizeKey', 'skipLine', 'typeData', 'location'\n                                , 'storageProperties', 'PartitionProvider', 'is_view', 'description']\n        df_tables = pd.DataFrame(all_table_metadata,  columns=table_metadata_columns)\n        # drop some columns we dont need\n        df_tables.drop(labels = ['crawlerDeserializerV', 'crawlerSerializarV', 'storageProperties'], axis=1, inplace=True)\n        # resolve missing values\n        df_tables.fillna(value={'avgRecordSize': 0, 'objectCount': 0, 'recordCount': 0, \n            'sizeKey': 0, 'skipLine': 0}, inplace=True)\n        df_tables.fillna('None', inplace=True)\n        return df_tables\n\n\n    def get_create_df(self) -> pd.DataFrame:\n        \"\"\"Return pandas databframe of create table statements given list of tables.\"\"\"\n        \n        all_create_metadata = []\n        for table in self.tables:\n            create = spark.sql(f\"SHOW CREATE TABLE {table['database']}.{table['tableName']}\").collect()[0][0]\n            table_metadata = [table['database'], table['tableName']] + [create]\n            all_create_metadata.append(table_metadata)\n        create_metadata_columns = ['database', 'table','createStatement']\n        df_create = pd.DataFrame(all_create_metadata, columns=create_metadata_columns)\n        return df_create\n\n    def temp_view_from_pandas(self, df, view_name):\n        \"\"\"Create a temporary view from a pandas dataframe\"\"\"\n\n        spark_df = spark.createDataFrame(df)\n        spark_df.createOrReplaceTempView(view_name)\n        print(f\"Temporary table `{view_name}` created.\")\n        print(f\"Row count: {spark_df.count()}\")\n        \n    def main(self) -> None:\n        \"Execute all of the workflow\"\n        # collect metadata and land metadata for schema\n        metadata_df = self.get_column_df()\n        self.temp_view_from_pandas(metadata_df, 'Column_Metadata')\n        metadata_df = self.get_table_df()\n        self.temp_view_from_pandas(metadata_df, 'Table_Metadata')\n        metadata_df = self.get_create_df()\n        self.temp_view_from_pandas(metadata_df, 'Create_Metadata')",
			"metadata": {
				"trusted": true
			},
			"execution_count": 6,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "Land_Metastore(['data_lake']).main()",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 7,
			"outputs": [
				{
					"name": "stdout",
					"text": "Temporary table `Column_Metadata` created.\nRow count: 54\nTemporary table `Table_Metadata` created.\nRow count: 4\nTemporary table `Create_Metadata` created.\nRow count: 4\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "class Upsert_Metadata():\n    \"Upsert metadata tables\"\n    \n    def __init__(self, catalog_name: str, database_name: str) -> None:\n        \"\"\"\n        initialize the database parameters\n        Arg:\n            catalog_name: the glue data catalog name\n            database_name: the glue database name of dictionary data\n        \"\"\"\n        \n        self.database_name = database_name\n        self.catalog_name = catalog_name\n        self.warehouse_path = f\"s3://{bucket_name}/{bucket_prefix}\"\n        \n    def upsert_column_data(self, column_table_name: str = 'column') -> None:\n        \"\"\"\n        Upsert dictionary table column with all of the available data\n        Arg:\n            column_table_name: the name of the dictionary column's table\n        Return:\n            None\n        \"\"\"\n        \n        spark.sql(f\"\"\"\n            MERGE INTO {catalog_name}.{database_name}.{column_table_name}  AS glue\n                USING Column_Metadata AS temp\n                ON glue.database = temp.database\n                AND glue.table = temp.table \n                AND glue.column_name = temp.ColumnName\n                WHEN MATCHED THEN UPDATE SET\n                glue.description=CASE WHEN temp.description = 'None' THEN NULL ELSE temp.description END,\n                glue.ordinal_position=temp.ordinalPosition,\n                glue.data_type=temp.dataType,\n                glue.nullable=temp.nullable,\n                glue.is_partitioned=temp.isPartitioned,\n                glue.is_deleted=False\n            WHEN NOT MATCHED THEN INSERT\n            (column_id, table_id, database, table, column_name\n            , description, ordinal_position, data_type, nullable,\n            is_partitioned, is_deleted)\n            VALUES\n                (CONCAT_WS('.', temp.database, temp.table, temp.columnName),\n                CONCAT_WS('.', temp.database, temp.Table),\n                temp.database,\n                temp.table,\n                temp.columnName,\n                CASE WHEN temp.description = 'None' THEN NULL ELSE temp.description END,\n                temp.ordinalPosition,\n                temp.dataType,\n                temp.nullable,\n                temp.isPartitioned,\n                False\n                )\n            \"\"\").show()\n        \n    def update_deleted_column(self, column_table_name: str = 'column') -> None:\n        \"\"\"\n        update delete column in the column's dictionary table\n        Arg:\n            column_table_name: the name of the dictionary column's table\n        Return:\n            None\n        \"\"\"\n\n        spark.sql(f\"\"\"CREATE OR REPLACE TEMP VIEW deleted_values AS\n            SELECT glue.*\n            FROM {catalog_name}.{database_name}.{column_table_name} glue\n            ANTI JOIN Column_Metadata temp\n                ON glue.database = temp.database\n                AND glue.column_name = temp.columnName\n                AND glue.table = temp.table\n            WHERE glue.database IN (SELECT database FROM Column_Metadata)\n            \"\"\").show()\n        spark.sql(f\"\"\"\n            MERGE INTO {catalog_name}.{database_name}.{column_table_name} glue\n            USING deleted_values deleted\n            ON glue.database = deleted.database\n                AND glue.column_name = deleted.column_name\n                AND glue.table = deleted.table\n            WHEN MATCHED THEN UPDATE SET\n            glue.is_deleted=True\n            \"\"\").show()\n            \n    def create_table_metadata(self) -> None:\n        \"\"\"\n        Create table's metadata\n        Arg:\n            column_table_name: the name of the dictionary table\n        Return:\n            None\n        \"\"\"\n\n        spark.sql(\"\"\"\n        CREATE OR REPLACE TEMP VIEW stage_table_metadata AS\n        SELECT \n            table.database || '.' || table.table AS table_id,\n            table.database AS database,\n            table.table AS table,\n            CASE WHEN table.description ='None' THEN null ELSE table.description END AS description,\n            metadata.createStatement as create_statement,\n            table.updateCrawler as update_crawler,\n            CASE WHEN table.areColumnsQuoted IN ('None', 'none') THEN false ELSE cast(table.areColumnsQuoted as boolean) END AS are_columns_quoted,\n            CASE WHEN table.classification IN ('None', 'none') THEN null ELSE table.classification END AS classification,\n            CASE WHEN table.columnsOrdered IN ('None', 'none') THEN false ELSE cast(table.columnsOrdered AS boolean) END AS columns_ordered,\n            CASE WHEN table.compressionType IN ('None', 'none') THEN null ELSE table.compressionType END AS compression_type,\n            CASE WHEN table.delimiter ='None' THEN null ELSE table.delimiter END AS delimiter,\n            cast(table.objectCount AS integer) AS object_count,\n            cast(table.recordCount AS integer) AS record_count,\n            cast(table.sizeKey AS integer) AS size_key,\n            cast(table.skipLine AS integer) AS skip_line,\n            table.typeData AS type_data,\n            table.location,\n            is_view,\n            False AS is_deleted\n        FROM table_metadata table\n        JOIN create_metadata metadata\n            ON table.database = metadata.database\n            AND table.table = metadata.table\n        \"\"\").show()\n            \n    def upsert_table_data(self, table_table_name: str = 'table') -> None:\n        \"\"\"\n        Upsert dictionary table table with all of the available data\n        Arg:\n            column_table_table: the name of the dictionary table's table\n        Return:\n            None\n        \"\"\"\n\n        spark.sql(f\"\"\"\n        MERGE INTO {catalog_name}.{database_name}.{table_table_name} glue \n        USING stage_table_metadata  temp\n            ON glue.database = temp.database\n            AND glue.table = temp.table \n        WHEN MATCHED THEN UPDATE SET\n            glue.create_statement = temp.create_statement,\n            glue.update_crawler = temp.update_crawler,\n            glue.are_columns_quoted = temp.are_columns_quoted,\n            glue.classification = temp.classification,\n            glue.columns_ordered = temp.columns_ordered,\n            glue.compression_type = temp.compression_type,\n            glue.delimiter = temp.delimiter,\n            glue.object_count = temp.object_count,\n            glue.record_count = temp.record_count,\n            glue.size_key = temp.size_key,\n            glue.type_data = temp.type_data,\n            glue.location = temp.location,\n            glue.is_view = temp.is_view,\n            glue.is_deleted = temp.is_deleted\n        WHEN NOT MATCHED THEN INSERT *\n        \"\"\").show()\n            \n    def update_delete_tables(self, table_table_name: str = 'table') -> None:\n        \"\"\"\n        Get delete tables\n        Arg:\n            table_table_table: the name of the dictionary table's table\n        Return:\n            None\n        \"\"\"\n\n        spark.sql(f\"\"\"\n        CREATE OR REPLACE TEMP VIEW deleted_values AS\n        SELECT table.*\n        FROM {catalog_name}.{database_name}.{table_table_name} table\n        ANTI JOIN table_metadata temp\n            ON table.database = temp.database\n            AND table.table = temp.table\n        \"\"\").show()\n        spark.sql(f\"\"\"\n        MERGE INTO {catalog_name}.{database_name}.{table_table_name} table\n        USING deleted_values temp\n        ON table.database = temp.database\n            AND table.table = temp.table\n        WHEN MATCHED THEN UPDATE SET\n            table.is_deleted=True\n        \"\"\").show()\n    \n    def upsert_tables(self) -> None:\n        \"Execute the upsert process\"\n        self.upsert_column_data()\n        self.update_deleted_column()\n        self.create_table_metadata()\n        self.upsert_table_data()\n        self.update_delete_tables()",
			"metadata": {
				"trusted": true
			},
			"execution_count": 11,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "Upsert_Metadata(catalog_name, database_name).upsert_tables()",
			"metadata": {},
			"execution_count": null,
			"outputs": []
		},
		{
			"cell_type": "markdown",
			"source": "## Reorder Tables",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "import boto3\nfrom time import sleep\n\nclass Update_Metadata:\n    \"\"\"\n    Update the metadata tables and add some descriptions\n    \"\"\"\n    \n    def __init__(self, catalog: str, dictionary_database: str, database: str, output_file: str) -> None:\n        \"\"\"\n        initialize the application\n        Arg:\n            catalog: the glue catalog name\n            dictionary_database: name of the quality database\n            database: the database that are going to the analyze\n            path_athena: the path when all of the athena output is going to be \"S3 path\"\n        return: None\n        \"\"\"\n        self.catalog = catalog\n        self.dictionary_database = dictionary_database\n        self.database = database\n        self.athena_client = boto3.client(service_name='athena')\n        self.output_file= output_file\n        \n    def reorder_columns_alphabetically(self, dictionary: str = 'column') -> None:\n        \"\"\"\n        reorder column table\n        Arg:\n            dictionary: the name of the column\n        return: None\n        \"\"\"\n        spark.sql(f\"\"\"CREATE OR REPLACE TEMP VIEW reorder_columns AS\n            SELECT table_id,\n            column_name, \n            ROW_NUMBER() OVER (PARTITION BY table_id ORDER BY column_name) AS new_ordinal\n            FROM {self.catalog}.{self.dictionary_database}.{dictionary} \n            WHERE LOWER(database) = '{self.database.lower()}'\n            \"\"\")\n        spark.sql(f\"\"\"MERGE INTO {self.catalog}.{self.dictionary_database}.{dictionary} old\n            USING reorder_columns AS new\n            ON new.column_name = old.column_name\n            AND new.table_id = old.table_id\n            WHEN MATCHED THEN UPDATE SET\n            old.ordinal_position = new.new_ordinal\"\"\")\n        spark.sql(\"DROP VIEW reorder_columns\")\n        \n    def update_database(self) -> None:\n        \"collect list of tables in database that are not temporary\"\n        tables = [i['tableName'] for i in spark.sql(f'SHOW TABLES FROM {self.database}').collect() if not i['isTemporary']]\n        print(f'Updating tables: {tables} \\n')\n        for table in tables:\n            self.update_table(table=table)\n            \n    def execute_query(self, query: str, output_file: str = None) -> None:\n        \"\"\"\n        Execute the query in Athena\n        Arg:\n            query: query to execute\n            output_file: path of the S3 output\n        \"\"\"\n        if output_file is None: output_file = self.output_file\n        query_id = athena_client.start_query_execution(QueryString=query.replace('glue_catalog.','')\n            , WorkGroup='primary',ResultConfiguration={'OutputLocation': output_file})['QueryExecutionId']\n        while True:\n            status = athena_client.get_query_execution(QueryExecutionId=query_id)['QueryExecution']['Status']['State']\n            if status != 'RUNNING': break \n            sleep(5)\n            \n    def update_table(self, table: str, dictionary : str = 'column') -> None:\n        \"\"\"\n        Use the data dictionary to determine the ordinal position and comment associated\n        with each column in a given table. Generate an alter table statement that applies\n        these changes to the table.\n        ########################################\n        \"\"\"\n      # check if table is view, skip if so.\n        if self.is_view(table):\n            print(f'Cannot update {self.database}.{table} because its a view. \\n')\n            return\n\n        df_column = spark.sql(f\"\"\"SELECT * FROM {self.catalog}.{self.dictionary_database}.{dictionary} \n            WHERE is_deleted = False\n            AND LOWER(database) = '{self.database.lower()}'\n            AND LOWER(table) = '{table.lower()}'\n            ORDER BY ordinal_position, column_name\"\"\").toPandas()\n\n        alter = []\n        for cn, dt,d in zip(df_column['column_name'], df_column['data_type'], df_column['description']):\n            if d:\n            # if description include comment\n                alter+= [f\"{cn} {dt} COMMENT '{d}'\"]\n            else:\n            # else dont include comment\n                alter+= [f\"`{cn}` {dt}\"]\n        alter = ', '.join(alter)\n\n        query = f\"ALTER TABLE {self.catalog}.{self.database}.{table} REPLACE COLUMNS ({alter})\"\n\n        print(query)\n        self.execute_query(query)\n\n\n    def is_view(self, table, dictionary = 'table'):\n        \"\"\"Check if a table exists, if so return True, if view return False.\"\"\"\n\n        df = spark.sql(f\"\"\"SELECT * \n            FROM {self.catalog}.{self.dictionary_database}.{dictionary}\n            WHERE LOWER(table) = '{table.lower()}'\n            AND LOWER (database) = '{self.database.lower()}'\n            AND is_deleted=False\"\"\").toPandas()\n        if df.shape[0]==0:\n            raise Exception(f'Table {table} does not exist in dictionary')\n\n        return df['is_view'][0]\n",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 79,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "Update_Metadata(catalog='glue_catalog', dictionary_database='dictionary_quality'\n    , database='data_lake', output_file='s3://andres-lagos-bucket/query-athena/').update_database()",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 80,
			"outputs": [
				{
					"name": "stdout",
					"text": "Updating tables: ['australia', 'kingdom', 'usa', 'zealand'] \n\nALTER TABLE glue_catalog.data_lake.australia REPLACE COLUMNS (`algorithm-marked-outlier` boolean, `comments` bigint, `event-id` bigint, `individual-local-identifier` string, `individual-taxon-canonical-name` string, `location-lat` double, `location-long` double, `manually-marked-outlier` string, `sensor-type` string, `study-name` string, `tag-local-identifier` string, `timestamp` string, `visible` boolean)\nALTER TABLE glue_catalog.data_lake.kingdom REPLACE COLUMNS (`algorithm-marked-outlier` boolean, `event-id` bigint, `ground-speed` double, `height-above-ellipsoid` double, `individual-local-identifier` string, `individual-taxon-canonical-name` string, `location-lat` double, `location-long` double, `manually-marked-outlier` boolean, `sensor-type` string, `study-name` string, `tag-local-identifier` string, `timestamp` string, `visible` boolean)\nALTER TABLE glue_catalog.data_lake.usa REPLACE COLUMNS (`algorithm-marked-outlier` boolean, `event-id` bigint, `ground-speed` double, `heading` double, `height-above-ellipsoid` double, `individual-local-identifier` string, `individual-taxon-canonical-name` string, `location-lat` double, `location-long` double, `manually-marked-outlier` string, `sensor-type` string, `study-name` string, `tag-local-identifier` string, `timestamp` string, `visible` boolean)\nALTER TABLE glue_catalog.data_lake.zealand REPLACE COLUMNS (`algorithm-marked-outlier` boolean, `event-id` bigint, `individual-local-identifier` string, `individual-taxon-canonical-name` string, `location-lat` double, `location-long` double, `manually-marked-outlier` boolean, `sensor-type` string, `study-name` string, `tag-local-identifier` string, `timestamp` string, `visible` boolean)\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "#catalog and dictionary_database\nUpdate_Metadata(catalog='glue_catalog', dictionary_database='dictionary_quality'\n    , database='data_lake', output_file='s3://andres-lagos-bucket/query-athena/').reorder_columns_alphabetically()",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 27,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "athena_client.get_query_execution(QueryExecutionId=query_id)['QueryExecution']['Status']['State']",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 59,
			"outputs": [
				{
					"name": "stdout",
					"text": "'RUNNING'\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "## Data Quality test",
			"metadata": {
				"tags": []
			}
		},
		{
			"cell_type": "code",
			"source": "from pyspark.sql import SparkSession, utils",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 13,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "try:\n    spark.sql(\"\"\"ALTER TABLE glue_catalog.dictionary_quality.column ADD COLUMNS null_test BOOLEAN, length_test INT\"\"\")\n    spark.sql(\"\"\"ALTER TABLE glue_catalog.dictionary_quality.table ADD COLUMNS Key STRING AFTER description\"\"\")\nexcept utils.AnalysisException:\n    print(\"Column already exits\")",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 15,
			"outputs": [
				{
					"name": "stdout",
					"text": "Column already exits\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "spark.sql(\"\"\"UPDATE glue_catalog.dictionary_quality.table\n    SET key = 'uuid \"fake\"'\n    WHERE table IN ('zealand', 'australia', 'usa', 'kingdom')\"\"\")\n",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 8,
			"outputs": [
				{
					"name": "stdout",
					"text": "DataFrame[]\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "spark.sql(\"\"\"SELECT * FROM glue_catalog.dictionary_quality.column\"\"\")",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 32,
			"outputs": [
				{
					"name": "stdout",
					"text": "DataFrame[column_id: string, table_id: string, database: string, table: string, column_name: string, description: string, data_type: string, nullable: boolean, is_partitioned: boolean, ordinal_position: int, is_deleted: boolean, length_test: int, null_test: boolean]\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "spark.sql(\"\"\"\nUPDATE glue_catalog.dictionary_quality.column\nSET null_test = True\nWHERE column_name IN ('visible','event-id')\n\"\"\").collect()",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 53,
			"outputs": [
				{
					"name": "stdout",
					"text": "[]\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "spark.sql(\"\"\"\nUPDATE glue_catalog.dictionary_quality.column\nSET length_test = 3\nWHERE column_name = 'sensor-type'\n\"\"\").collect()",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 56,
			"outputs": [
				{
					"name": "stdout",
					"text": "[]\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "spark.sql(\"\"\"SELECT database, table, column_name, length_test, \n        CONCAT(column_id,'.','is_not_length_', length_test) AS test_name\n    FROM glue_catalog.dictionary_quality.column \n    where length_test IS NOT null AND is_deleted=false\"\"\")\ndf_null_test.createOrReplaceTempView(\"length_test\")\nspark.sql(\"SELECT * FROM length_test\").show()",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 60,
			"outputs": [
				{
					"name": "stdout",
					"text": "+---------+---------+-----------+--------------------+\n| database|    table|column_name|           test_name|\n+---------+---------+-----------+--------------------+\n|data_lake|australia|   event-id|data_lake.austral...|\n|data_lake|australia|    visible|data_lake.austral...|\n|data_lake|  kingdom|    visible|data_lake.kingdom...|\n|data_lake|  kingdom|   event-id|data_lake.kingdom...|\n|data_lake|      usa|   event-id|data_lake.usa.eve...|\n|data_lake|      usa|    visible|data_lake.usa.vis...|\n|data_lake|  zealand|   event-id|data_lake.zealand...|\n|data_lake|  zealand|    visible|data_lake.zealand...|\n+---------+---------+-----------+--------------------+\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "df_null_test = spark.sql(\"\"\"SELECT database, table, column_name\n        , CONCAT_WS('.',column_id,'is_null') AS test_name\n    FROM glue_catalog.dictionary_quality.column\n    WHERE null_test=true\n    AND is_deleted=false\"\"\")\ndf_null_test.createOrReplaceTempView(\"null_test\")\nspark.sql(\"SELECT * FROM null_test\").show()",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 59,
			"outputs": [
				{
					"name": "stdout",
					"text": "+---------+---------+-----------+--------------------+\n| database|    table|column_name|           test_name|\n+---------+---------+-----------+--------------------+\n|data_lake|australia|   event-id|data_lake.austral...|\n|data_lake|australia|    visible|data_lake.austral...|\n|data_lake|  kingdom|    visible|data_lake.kingdom...|\n|data_lake|  kingdom|   event-id|data_lake.kingdom...|\n|data_lake|      usa|   event-id|data_lake.usa.eve...|\n|data_lake|      usa|    visible|data_lake.usa.vis...|\n|data_lake|  zealand|   event-id|data_lake.zealand...|\n|data_lake|  zealand|    visible|data_lake.zealand...|\n+---------+---------+-----------+--------------------+\n",
					"output_type": "stream"
				}
			]
		}
	]
}